title: Test1
---
author: Eric J. Ma
---
body:

I recently learned how to deploy [Ollama](https://ollama.com/) to [Modal](https://modal.com/)! I mostly copied code from another source but modified it just enough that I think I have upgraded my mental model of Modal and want to leave notes. My motivation here was to gain access to open source models that are larger than can fit comfortably on my 16GB M1 MacBook Air.

My next steps are to figure out how to make this work with [LlamaBot](https://github.com/ericmjl/llamabot). I haven't yet figured out if it's possible to make the endpoint URL match the OpenAI format, in which case it'll be dead simple to drop in my Ollama-on-Modal endpoint onto [LiteLLM](https://github.com/BerriAI/litellm). However, if that's not possible, then my backup plan is to figure out a way to proxy requests to the Ollama-on-Modal endpoint with minimal configuration.

---
pub_date: 2024-11-18
---
twitter_handle: ericmjl
---
summary: In this blog post, I share my journey of deploying Ollama to Modal, enhancing my understanding of Modal's capabilities. I detail the script used, the setup of the Modal app, and the deployment process, which includes ensuring the Ollama service is ready and operational. This exploration not only expanded my technical skills but also prepared me for future integrations with larger models. Curious about how this deployment could streamline your projects?
---
tags:

modal
deployment
